{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, n_estimators: int = 100, lr: float = 0.5, ndcg_top_k: int = 10,\n",
    "                 subsample: float = 0.6, colsample_bytree: float = 0.9,\n",
    "                 max_depth: int = 5, min_samples_leaf: int = 8):\n",
    "        self._prepare_data()\n",
    "\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "        # допишите ваш код здесь\n",
    "\n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "                                        inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "        # допишите ваш код здесь \n",
    "        pass\n",
    "\n",
    "    def _train_one_tree(self, cur_tree_idx: int,\n",
    "                        train_preds: torch.FloatTensor\n",
    "                        ) -> Tuple[DecisionTreeRegressor, np.ndarray]:\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def _calc_data_ndcg(self, queries_list: np.ndarray,\n",
    "                        true_labels: torch.FloatTensor, preds: torch.FloatTensor) -> float:\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        np.random.seed(0)\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def predict(self, data: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def _compute_lambdas(self, y_true: torch.FloatTensor, y_pred: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def _ndcg_k(self, ys_true, ys_pred, ndcg_top_k) -> float:\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        # допишите ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        # допишите ваш код здесь\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code to send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, n_estimators: int = 100, lr: float = 0.5, ndcg_top_k: int = 10,\n",
    "                 subsample: float = 0.6, colsample_bytree: float = 0.9,\n",
    "                 max_depth: int = 5, min_samples_leaf: int = 8):\n",
    "        self._prepare_data()\n",
    "\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "        # допишите ваш код здесь\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        \n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "        \n",
    "        # scale train data\n",
    "        self.X_train = torch.FloatTensor(self._scale_features_in_query_groups(\n",
    "            inp_feat_array=X_train, inp_query_ids=self.query_ids_train))\n",
    "#         self.ys_train = torch.FloatTensor(y_train).t()\n",
    "        self.ys_train = torch.FloatTensor(y_train).reshape(-1,1)\n",
    "        \n",
    "        # scale test data\n",
    "        self.X_test = torch.FloatTensor(self._scale_features_in_query_groups(\n",
    "            inp_feat_array=X_test, inp_query_ids=self.query_ids_test))\n",
    "#         self.ys_test = torch.FloatTensor(y_test).t()\n",
    "        self.ys_test = torch.FloatTensor(y_test).reshape(-1,1)\n",
    "        pass\n",
    "\n",
    "    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "                                        inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "        # get unique ids\n",
    "        inp_query_ids_uniq = np.unique(inp_query_ids)\n",
    "        \n",
    "        # scale each group by id\n",
    "        for id_i in inp_query_ids_uniq:\n",
    "            scaler = StandardScaler()\n",
    "            inp_feat_array[inp_query_ids == id_i, :] = \\\n",
    "                scaler.fit_transform(inp_feat_array[inp_query_ids == id_i, :])\n",
    "\n",
    "        return inp_feat_array\n",
    "\n",
    "    \n",
    "    \n",
    "    def _train_one_tree(self, cur_tree_idx: int,\n",
    "                        train_preds: torch.FloatTensor\n",
    "                        ) -> Tuple[DecisionTreeRegressor, np.ndarray]:\n",
    "        # допишите ваш код здесь\n",
    "        random.seed(cur_tree_idx)\n",
    "        \n",
    "        ## compute lambdas with train_preds\n",
    "        ids_uniq = np.unique(self.query_ids_train)\n",
    "        \n",
    "#         print('self.train_preds')\n",
    "#         print(self.train_preds)\n",
    "#         print('np.shape(self.train_preds)')\n",
    "#         print(np.shape(self.train_preds))\n",
    "        if all(np.array(self.train_preds) == 0):\n",
    "            lambdas_upd = self.ys_train\n",
    "        else:\n",
    "            lambdas_list = []\n",
    "\n",
    "            for n_id in ids_uniq:\n",
    "\n",
    "                X_train_id = self.X_train[self.query_ids_train == n_id]\n",
    "                ys_train_id = self.ys_train[self.query_ids_train == n_id]\n",
    "                train_preds_id = train_preds[self.query_ids_train == n_id]\n",
    "\n",
    "                lambdas_list.append(\\\n",
    "                    self._compute_lambdas(y_true=ys_train_id, y_pred=train_preds_id))\n",
    "\n",
    "#             lambdas_upd = np.concatenate(lambdas_list)\n",
    "            lambdas_upd = np.concatenate(lambdas_list) * -1\n",
    "#             lambdas_upd = np.concatenate(lambdas_list) * -1 * self.lr\n",
    "        \n",
    "        ## set train data\n",
    "        N, M = np.shape(self.X_train)\n",
    "        \n",
    "        random.seed(cur_tree_idx)\n",
    "        sample_ids = random.sample(range(N), int(N * self.subsample))\n",
    "        random.seed(cur_tree_idx)\n",
    "        colsample_ids = np.array(random.sample(range(M), int(M * self.colsample_bytree)))\n",
    "        \n",
    "#         print('np.shape(self.X_train)')\n",
    "#         print(np.shape(self.X_train))\n",
    "#         print('np.shape(sample_ids)')\n",
    "#         print(np.shape(sample_ids))\n",
    "#         print('np.shape(colsample_ids)')\n",
    "#         print(np.shape(colsample_ids))\n",
    "        X_train = self.X_train[sample_ids, :][:, colsample_ids]\n",
    "#         ys_train = self.ys_train[sample_ids, ]\n",
    "#         ys_train = train_preds[sample_ids]\n",
    "        ys_train = lambdas_upd[sample_ids]\n",
    "        \n",
    "#         print('ys_train_train_one_tree')\n",
    "#         print(ys_train)\n",
    "        \n",
    "        # fit one tree & predict\n",
    "        single_tree = DecisionTreeRegressor(max_depth=self.max_depth,\\\n",
    "                                            min_samples_leaf=self.min_samples_leaf,\\\n",
    "                                            random_state=cur_tree_idx)\n",
    "        single_tree.fit(X=X_train, y=ys_train)\n",
    "        \n",
    "        # return tree and features ids\n",
    "        return (single_tree, colsample_ids)\n",
    "\n",
    "    \n",
    "#     def _compute_lambdas(self, y_true: torch.FloatTensor, y_pred: torch.FloatTensor) -> torch.FloatTensor:\n",
    "#         # допишите ваш код здесь\n",
    "#         pass\n",
    "    \n",
    "    def _compute_lambdas(self, y_true, y_pred, ndcg_scheme='exp2'):\n",
    "        # рассчитаем нормировку, IdealDCG\n",
    "#         print('y_true')\n",
    "#         print(y_true)\n",
    "#         print('y_pred')\n",
    "#         print(y_pred)\n",
    "        if all(np.array(y_true) == 0):\n",
    "            ideal_dcg = 1\n",
    "        else:\n",
    "            ideal_dcg = self._compute_ideal_dcg(y_true.reshape(1,-1)[0],\\\n",
    "                                            ndcg_scheme=ndcg_scheme)\n",
    "        \n",
    "#         print('ideal_dcg')\n",
    "#         print(ideal_dcg)\n",
    "        N = 1 / ideal_dcg\n",
    "\n",
    "        # рассчитаем порядок документов согласно оценкам релевантности\n",
    "        _, rank_order = torch.sort(y_true, descending=True, axis=0)\n",
    "        rank_order += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # получаем все попарные разницы скоров в батче\n",
    "#             pos_pairs_score_diff = 1.0 + torch.exp((y_pred - y_pred.t()))\n",
    "            pos_pairs_score_diff = 1.0 + torch.exp((y_pred - y_pred.reshape(-1,1)))\n",
    "#             print('pos_pairs_score_diff')\n",
    "#             print(pos_pairs_score_diff)\n",
    "            \n",
    "            # поставим разметку для пар, 1 если первый документ релевантнее\n",
    "            # -1 если второй документ релевантнее\n",
    "            Sij = self._compute_labels_in_batch(y_true)\n",
    "#             print('Sij')\n",
    "#             print(Sij)\n",
    "            # посчитаем изменение gain из-за перестановок\n",
    "            gain_diff = self._compute_gain_diff(y_true, ndcg_scheme)\n",
    "#             print('gain_diff')\n",
    "#             print(gain_diff)\n",
    "            \n",
    "            # посчитаем изменение знаменателей-дискаунтеров\n",
    "#             decay_diff = (1.0 / torch.log2(rank_order + 1.0)) - (1.0 / torch.log2(rank_order.t() + 1.0))\n",
    "            decay_diff = (1.0 / torch.log2(rank_order + 1.0)) - \\\n",
    "                (1.0 / torch.log2(rank_order.reshape(1,-1) + 1.0))\n",
    "#             print('decay_diff')\n",
    "#             print(decay_diff)\n",
    "            # посчитаем непосредственное изменение nDCG\n",
    "            delta_ndcg = torch.abs(N * gain_diff * decay_diff)\n",
    "            # посчитаем лямбды\n",
    "            lambda_update =  (0.5 * (1 - Sij) - 1 / pos_pairs_score_diff) * delta_ndcg\n",
    "#             print('lambda_update')\n",
    "#             print(lambda_update)\n",
    "            lambda_update = torch.sum(lambda_update, dim=1, keepdim=True)\n",
    "\n",
    "#             return Sij, gain_diff, decay_diff, delta_ndcg, lambda_update\n",
    "        return lambda_update\n",
    "    \n",
    "    def _compute_labels_in_batch(self, y_true):\n",
    "\n",
    "        # разница релевантностей каждого с каждым объектом\n",
    "#         print('y_true_in_compute_labels_in_batch')\n",
    "#         print(y_true)\n",
    "#         rel_diff = y_true - y_true.t()\n",
    "        rel_diff = y_true - y_true.reshape(1,-1)\n",
    "#         print('rel_diff')\n",
    "#         print(rel_diff)\n",
    "        \n",
    "        # 1 в этой матрице - объект более релевантен\n",
    "        pos_pairs = (rel_diff > 0).type(torch.float32)\n",
    "\n",
    "        # 1 тут - объект менее релевантен\n",
    "        neg_pairs = (rel_diff < 0).type(torch.float32)\n",
    "        Sij = pos_pairs - neg_pairs\n",
    "        return Sij\n",
    "\n",
    "    def _compute_gain_diff(self, y_true, gain_scheme):\n",
    "        if gain_scheme == \"exp2\":\n",
    "#             gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.t())\n",
    "            gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.reshape(1,-1))\n",
    "        elif gain_scheme == \"diff\":\n",
    "            gain_diff = y_true - y_true.t()\n",
    "        else:\n",
    "            raise ValueError(f\"{gain_scheme} method not supported\")\n",
    "        return gain_diff\n",
    "    \n",
    "    def _calc_data_ndcg(self, queries_list: np.ndarray,\n",
    "                        true_labels: torch.FloatTensor, preds: torch.FloatTensor) -> float:\n",
    "        \n",
    "        ndcgs = []\n",
    "        \n",
    "        # допишите ваш код здесь\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             ids_test_uniq = np.unique(self.query_ids_test)\n",
    "\n",
    "#             for id_test in ids_test_uniq:\n",
    "#                 cur_X_test = self.X_test[self.query_ids_test == id_test, :]\n",
    "#                 valid_pred = self.predict(cur_X_test)\n",
    "#                 cur_ndcg = self._ndcg_k(\\\n",
    "#                     ys_true=self.ys_test.reshape(1,-1)[0][self.query_ids_test == id_test],\\\n",
    "#                     ys_pred=valid_pred,\\\n",
    "#                     ndcg_top_k=10)\n",
    "                \n",
    "            ids_test_uniq = np.unique(queries_list)\n",
    "\n",
    "            for id_test in ids_test_uniq:\n",
    "                cur_ndcg = self._ndcg_k(\\\n",
    "                    ys_true=true_labels[queries_list == id_test],\\\n",
    "                    ys_pred=preds[queries_list == id_test],\\\n",
    "                    ndcg_top_k=self.ndcg_top_k)\n",
    "                \n",
    "                ndcgs.append(cur_ndcg)\n",
    "                \n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def fit(self):\n",
    "        np.random.seed(0)\n",
    "        # допишите ваш код здесь\n",
    "        self.train_preds = torch.zeros(np.shape(self.ys_train)[0])\n",
    "        self.features_ids = []\n",
    "        self.trees = []\n",
    "        self.best_ndcg = 0\n",
    "        self.best_n_trees = 1\n",
    "        self.ndcg_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            single_tree, features_ids = self._train_one_tree(\\\n",
    "                cur_tree_idx=i, train_preds=self.train_preds)\n",
    "            self.trees.append(single_tree)\n",
    "            self.features_ids.append(features_ids)\n",
    "            \n",
    "            ## calculate new tree prediction & update train_preds\n",
    "            new_ys_preds = single_tree.predict(self.X_train[:, features_ids])\n",
    "#             self.train_preds += new_ys_preds\n",
    "#             self.train_preds += new_ys_preds * self.lr\n",
    "            \n",
    "#             print('new_ys_preds')\n",
    "#             print(new_ys_preds)\n",
    "#             print('self.train_preds')\n",
    "#             print(self.train_preds)\n",
    "            \n",
    "            ## calc new ndcg and update best & best_n_tree with _calc_data_ndcg\n",
    "            \n",
    "            # apply _calc_data_ndcg\n",
    "            ndcg_cur_train = self._calc_data_ndcg(\\\n",
    "                queries_list=self.query_ids_train, true_labels=self.ys_train,\\\n",
    "                preds=self.predict(self.X_train))\n",
    "            \n",
    "            ndcg_cur_test = self._calc_data_ndcg(\\\n",
    "                queries_list=self.query_ids_test, true_labels=self.ys_test,\\\n",
    "                preds=self.predict(self.X_test))\n",
    "            \n",
    "            ndcg_cur = ndcg_cur_test\n",
    "            self.ndcg_list.append(ndcg_cur)\n",
    "            self.best_ndcg = max(self.ndcg_list)\n",
    "            self.best_n_trees = np.argmax(self.ndcg_list) + 1\n",
    "            \n",
    "# #             print('i_in_fit_fun')\n",
    "#             print(i)\n",
    "# #             print('ndcg_cur_in_fit_fun')\n",
    "#             print(ndcg_cur)\n",
    "            print(\"n_tree: {}, train_ndcg_k: {}, test_ndcg_k: {}\".format(\\\n",
    "                i, ndcg_cur_train, ndcg_cur_test))\n",
    "        \n",
    "        self.trees = self.trees[:self.best_n_trees]\n",
    "        self.features_ids = self.features_ids[:self.best_n_trees]\n",
    "        self.n_estimators = self.best_n_trees\n",
    "        \n",
    "#         self.trees = trees_list\n",
    "\n",
    "    def predict(self, data: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # допишите ваш код здесь\n",
    "        preds = torch.zeros(np.shape(data)[0])\n",
    "#         for i in range(self.n_estimators):\n",
    "        for i in range(len(self.trees)):\n",
    "#             print('len(self.trees)')\n",
    "#             print(len(self.trees))\n",
    "            ## calculate new tree prediction & update train_preds\n",
    "#             print(\"n_estimator_i\")\n",
    "#             print(i)\n",
    "            new_ys_preds = self.trees[i].predict(data[:, self.features_ids[i]])\n",
    "#             self.train_preds += new_ys_preds * self.lr\n",
    "            preds += new_ys_preds * self.lr\n",
    "            \n",
    "        return preds\n",
    "            \n",
    "#     def _ndcg_k(self, ys_true, ys_pred, ndcg_top_k) -> float:\n",
    "#         # допишите ваш код здесь\n",
    "#         pass\n",
    "    \n",
    "    def _ndcg_k(self, ys_true: torch.Tensor, ys_pred: torch.Tensor,\n",
    "                ndcg_top_k: int) -> float:\n",
    "        # допишите ваш код здесь\n",
    "        ys_true = torch.reshape(ys_true, (-1,))\n",
    "        ys_pred = torch.reshape(ys_pred, (-1,))\n",
    "\n",
    "        ys_pred_sorted = torch.sort(ys_pred, descending=True)\n",
    "        ys_true_sorted = ys_true[ys_pred_sorted[1]]\n",
    "        ys_true_sorted_sep = torch.sort(ys_true, descending=True)[0]\n",
    "\n",
    "        dcg_act = self._dcg(ys_true=ys_true_sorted[:self.ndcg_top_k],\\\n",
    "                            ys_pred=ys_pred_sorted[0][:self.ndcg_top_k],\\\n",
    "                            gain_scheme='exp2')\n",
    "\n",
    "        dcg_max = self._dcg(ys_true=ys_true_sorted_sep[:self.ndcg_top_k],\\\n",
    "                            ys_pred=ys_true_sorted_sep[:self.ndcg_top_k],\\\n",
    "                            gain_scheme='exp2')\n",
    "\n",
    "#         try:\n",
    "#             ndcg_k = dcg_act / dcg_max\n",
    "#         except:\n",
    "#             ndcg_k = 0\n",
    "#         if np.isnan(ndcg_k):\n",
    "#             ndcg_k = 0\n",
    "        \n",
    "#         if (dcg_act == 0):\n",
    "#             ndcg_k = 0\n",
    "        if (dcg_max == 0):\n",
    "            ndcg_k = 1\n",
    "        else:\n",
    "            ndcg_k = dcg_act / dcg_max\n",
    "        \n",
    "        return ndcg_k\n",
    "\n",
    "    def _ndcg(self, ys_true: torch.Tensor, ys_pred: torch.Tensor, gain_scheme: str = 'const') -> float:\n",
    "#         try:\n",
    "        dcg_val = self._dcg(ys_true=ys_true, ys_pred=ys_pred, gain_scheme=gain_scheme)\n",
    "        ideal_dcg_val = self._dcg(ys_true=ys_true, ys_pred=ys_true, gain_scheme=gain_scheme)\n",
    "        \n",
    "#         print('dcg_val')\n",
    "#         print(dcg_val)\n",
    "#         print('ideal_dcg_val')\n",
    "#         print(ideal_dcg_val)\n",
    "        \n",
    "#         if (dcg_val == 0):\n",
    "#             return dcg_val\n",
    "        if (ideal_dcg_val == 0):\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return dcg_val / ideal_dcg_val\n",
    "    \n",
    "    def _compute_gain(self, y_value: float, gain_scheme: str) -> float:\n",
    "#         return 2 ** y_value - 1.\n",
    "        if gain_scheme == 'exp2':\n",
    "            return 2 ** y_value - 1.\n",
    "        else:\n",
    "            return y_value + 0.\n",
    "\n",
    "    def _dcg(self, ys_true: torch.Tensor, ys_pred: torch.Tensor, gain_scheme: str) -> float:\n",
    "        ys_pred_sorted = torch.sort(ys_pred, descending=True)\n",
    "#         print('ys_pred_sorted')\n",
    "#         print(ys_pred_sorted)\n",
    "        \n",
    "        log2_list = [math.log2(x) for x in range(2, len(ys_pred) + 2)]\n",
    "\n",
    "        dcg_val = 0.\n",
    "        for i in range(len(log2_list)):\n",
    "            dcg_val += self._compute_gain(ys_true[ys_pred_sorted[1]][i].item(), \\\n",
    "                                          gain_scheme=gain_scheme) / log2_list[i]\n",
    "            \n",
    "#         print('dcg_val')\n",
    "#         print(dcg_val)\n",
    "        \n",
    "#         if (dcg_val == 0):\n",
    "# #             dcg_val = -1\n",
    "#             dcg_val = 0.00001\n",
    "        return dcg_val\n",
    "    \n",
    "    def _compute_ideal_dcg(self, y_true, ndcg_scheme='exp2'):\n",
    "#         print(y_true)\n",
    "#         try:\n",
    "        return self._dcg(ys_true=y_true, ys_pred=y_true, gain_scheme=ndcg_scheme)\n",
    "#         except:\n",
    "#             return 1\n",
    "    \n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        # допишите ваш код здесь\n",
    "        ## save model\n",
    "        state = {\n",
    "            'trees': self.trees, \n",
    "            'features_ids': self.features_ids,\n",
    "            'ndcg_top_k': self.ndcg_top_k,\n",
    "#             'best_n_trees': self.best_n_trees,\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'lr': self.lr}\n",
    "        \n",
    "        f = open(path, 'wb')\n",
    "        pickle.dump(state, f)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        # допишите ваш код здесь\n",
    "        f = open(path, 'rb')\n",
    "        state = pickle.load(f)\n",
    "        \n",
    "        self.trees = state['trees']\n",
    "        self.features_ids = state['features_ids']\n",
    "        self.ndcg_top_k = state['ndcg_top_k']\n",
    "#         self.best_n_trees = state['best_n_trees']\n",
    "        self.lr = state['lr']\n",
    "        \n",
    "#         return state\n",
    "        pass\n",
    "    \n",
    "#     def save_obj(obj, name):\n",
    "#         with open(name + '.pkl', 'wb') as f:\n",
    "#             pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#     def load_obj(name):\n",
    "#         with open(name + '.pkl', 'rb') as f:\n",
    "#             return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_sol1 = Solution(n_estimators=100)\n",
    "# tst_sol1 = Solution(n_estimators=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tree: 0, train_ndcg_k: 0.45968059872384603, test_ndcg_k: 0.3295782293708683\n",
      "n_tree: 1, train_ndcg_k: 0.5216392964861525, test_ndcg_k: 0.336569784676915\n",
      "n_tree: 2, train_ndcg_k: 0.5221855200242471, test_ndcg_k: 0.36380267274701583\n",
      "n_tree: 3, train_ndcg_k: 0.5368954236716924, test_ndcg_k: 0.37187789290166084\n",
      "n_tree: 4, train_ndcg_k: 0.5403321922419148, test_ndcg_k: 0.3704574068159138\n",
      "n_tree: 5, train_ndcg_k: 0.5453128913862544, test_ndcg_k: 0.3725899192169184\n",
      "n_tree: 6, train_ndcg_k: 0.5485454855826337, test_ndcg_k: 0.3870515435122703\n",
      "n_tree: 7, train_ndcg_k: 0.5580627654957582, test_ndcg_k: 0.3934614396866454\n",
      "n_tree: 8, train_ndcg_k: 0.5559738142964052, test_ndcg_k: 0.40327906989800755\n",
      "n_tree: 9, train_ndcg_k: 0.5661915736895955, test_ndcg_k: 0.4010444691582316\n",
      "n_tree: 10, train_ndcg_k: 0.5690039433770224, test_ndcg_k: 0.4023557874145436\n",
      "n_tree: 11, train_ndcg_k: 0.5679965689200871, test_ndcg_k: 0.40419252489533336\n",
      "n_tree: 12, train_ndcg_k: 0.5631563454983873, test_ndcg_k: 0.40618933262105084\n",
      "n_tree: 13, train_ndcg_k: 0.5728869529274273, test_ndcg_k: 0.4175436060221069\n",
      "n_tree: 14, train_ndcg_k: 0.5707087640401215, test_ndcg_k: 0.4135271935589219\n",
      "n_tree: 15, train_ndcg_k: 0.5780507020514135, test_ndcg_k: 0.4191808371118613\n",
      "n_tree: 16, train_ndcg_k: 0.5734582730188102, test_ndcg_k: 0.4228227096681131\n",
      "n_tree: 17, train_ndcg_k: 0.577642676036845, test_ndcg_k: 0.42686814724940697\n",
      "n_tree: 18, train_ndcg_k: 0.5739583962621793, test_ndcg_k: 0.4309147410394726\n",
      "n_tree: 19, train_ndcg_k: 0.577700605669814, test_ndcg_k: 0.43173542982162266\n",
      "n_tree: 20, train_ndcg_k: 0.5761765190667532, test_ndcg_k: 0.4228880506639172\n",
      "n_tree: 21, train_ndcg_k: 0.5757390328787488, test_ndcg_k: 0.42334535818894725\n",
      "n_tree: 22, train_ndcg_k: 0.5741624776669215, test_ndcg_k: 0.42608281957318395\n",
      "n_tree: 23, train_ndcg_k: 0.5743556342078827, test_ndcg_k: 0.4240928710086643\n",
      "n_tree: 24, train_ndcg_k: 0.5785548959293515, test_ndcg_k: 0.42657496882023904\n",
      "n_tree: 25, train_ndcg_k: 0.5805674326360888, test_ndcg_k: 0.4278554963394976\n",
      "n_tree: 26, train_ndcg_k: 0.5769208847001036, test_ndcg_k: 0.42846230478843805\n",
      "n_tree: 27, train_ndcg_k: 0.5756143266420652, test_ndcg_k: 0.4286595407597224\n",
      "n_tree: 28, train_ndcg_k: 0.5785069300377532, test_ndcg_k: 0.4291813284948285\n",
      "n_tree: 29, train_ndcg_k: 0.5779053643436438, test_ndcg_k: 0.4278183997814921\n",
      "n_tree: 30, train_ndcg_k: 0.5742617284537197, test_ndcg_k: 0.4256892725356844\n",
      "n_tree: 31, train_ndcg_k: 0.574782349583976, test_ndcg_k: 0.4257911871290615\n",
      "n_tree: 32, train_ndcg_k: 0.5745517594071573, test_ndcg_k: 0.42718379023531855\n",
      "n_tree: 33, train_ndcg_k: 0.5708410809993042, test_ndcg_k: 0.4258695322165409\n",
      "n_tree: 34, train_ndcg_k: 0.5745522148894703, test_ndcg_k: 0.4280142138570468\n",
      "n_tree: 35, train_ndcg_k: 0.5727746345258, test_ndcg_k: 0.4185029157672239\n",
      "n_tree: 36, train_ndcg_k: 0.574938911191616, test_ndcg_k: 0.41920470212256505\n",
      "n_tree: 37, train_ndcg_k: 0.5730510713316382, test_ndcg_k: 0.4172235925226172\n",
      "n_tree: 38, train_ndcg_k: 0.5737344765184468, test_ndcg_k: 0.4191956566088516\n",
      "n_tree: 39, train_ndcg_k: 0.5769078354912472, test_ndcg_k: 0.4223783424919612\n",
      "n_tree: 40, train_ndcg_k: 0.5763480759990891, test_ndcg_k: 0.4234656986111274\n",
      "n_tree: 41, train_ndcg_k: 0.5766883810321332, test_ndcg_k: 0.42475430517628426\n",
      "n_tree: 42, train_ndcg_k: 0.5762227440198452, test_ndcg_k: 0.4265243285181776\n",
      "n_tree: 43, train_ndcg_k: 0.5781146667778553, test_ndcg_k: 0.423366440793493\n",
      "n_tree: 44, train_ndcg_k: 0.5788234660620055, test_ndcg_k: 0.42462333896582666\n",
      "n_tree: 45, train_ndcg_k: 0.5771771373899169, test_ndcg_k: 0.42274265713008546\n",
      "n_tree: 46, train_ndcg_k: 0.5750977415606243, test_ndcg_k: 0.42336565478824\n",
      "n_tree: 47, train_ndcg_k: 0.5741424191926628, test_ndcg_k: 0.42331809401441345\n",
      "n_tree: 48, train_ndcg_k: 0.5724908895427858, test_ndcg_k: 0.4179686834634133\n",
      "n_tree: 49, train_ndcg_k: 0.570609083492679, test_ndcg_k: 0.42155901913482724\n",
      "n_tree: 50, train_ndcg_k: 0.5726490086891095, test_ndcg_k: 0.42381594834133\n",
      "n_tree: 51, train_ndcg_k: 0.5719884082118066, test_ndcg_k: 0.42351779528729416\n",
      "n_tree: 52, train_ndcg_k: 0.571518709149115, test_ndcg_k: 0.4212071874312755\n",
      "n_tree: 53, train_ndcg_k: 0.5735343647662948, test_ndcg_k: 0.42893761168572814\n",
      "n_tree: 54, train_ndcg_k: 0.573342829708781, test_ndcg_k: 0.42994541657406643\n",
      "n_tree: 55, train_ndcg_k: 0.5743913747087888, test_ndcg_k: 0.4296256770881603\n",
      "n_tree: 56, train_ndcg_k: 0.5755417805185611, test_ndcg_k: 0.43073499397790693\n",
      "n_tree: 57, train_ndcg_k: 0.5782047292743839, test_ndcg_k: 0.4316104366516588\n",
      "n_tree: 58, train_ndcg_k: 0.576063676753902, test_ndcg_k: 0.4294916921360151\n",
      "n_tree: 59, train_ndcg_k: 0.573358661436805, test_ndcg_k: 0.4286854381159663\n",
      "n_tree: 60, train_ndcg_k: 0.5728287434067598, test_ndcg_k: 0.4289626968925246\n",
      "n_tree: 61, train_ndcg_k: 0.5723198816986483, test_ndcg_k: 0.42820485067738673\n",
      "n_tree: 62, train_ndcg_k: 0.5718979538642, test_ndcg_k: 0.4268542003202303\n",
      "n_tree: 63, train_ndcg_k: 0.5727142482280392, test_ndcg_k: 0.4265467964304435\n",
      "n_tree: 64, train_ndcg_k: 0.5752242554429196, test_ndcg_k: 0.4266479626854032\n",
      "n_tree: 65, train_ndcg_k: 0.5771061809152525, test_ndcg_k: 0.42656243902452096\n",
      "n_tree: 66, train_ndcg_k: 0.5781126940176796, test_ndcg_k: 0.42520606313024056\n",
      "n_tree: 67, train_ndcg_k: 0.577364373117062, test_ndcg_k: 0.42382520416176583\n",
      "n_tree: 68, train_ndcg_k: 0.5764442483266135, test_ndcg_k: 0.423704612182954\n",
      "n_tree: 69, train_ndcg_k: 0.5760594290413096, test_ndcg_k: 0.4241938936477905\n",
      "n_tree: 70, train_ndcg_k: 0.5747025225396546, test_ndcg_k: 0.42184274468402505\n",
      "n_tree: 71, train_ndcg_k: 0.5735803038118197, test_ndcg_k: 0.42340188883318447\n",
      "n_tree: 72, train_ndcg_k: 0.575161433028393, test_ndcg_k: 0.42292596482219674\n",
      "n_tree: 73, train_ndcg_k: 0.5771294646888185, test_ndcg_k: 0.420372361909395\n",
      "n_tree: 74, train_ndcg_k: 0.5762836055495368, test_ndcg_k: 0.42176903884438455\n",
      "n_tree: 75, train_ndcg_k: 0.5774429491401419, test_ndcg_k: 0.42010381867671387\n",
      "n_tree: 76, train_ndcg_k: 0.5780247273926903, test_ndcg_k: 0.4203001587515645\n",
      "n_tree: 77, train_ndcg_k: 0.5778049326093609, test_ndcg_k: 0.4233738388816716\n",
      "n_tree: 78, train_ndcg_k: 0.5794341190947626, test_ndcg_k: 0.4243316532619591\n",
      "n_tree: 79, train_ndcg_k: 0.579269188548249, test_ndcg_k: 0.4253821172388539\n",
      "n_tree: 80, train_ndcg_k: 0.5784020660207967, test_ndcg_k: 0.42181715170261286\n",
      "n_tree: 81, train_ndcg_k: 0.578076194360263, test_ndcg_k: 0.42392964687216955\n",
      "n_tree: 82, train_ndcg_k: 0.5776379745831853, test_ndcg_k: 0.4240052572643302\n",
      "n_tree: 83, train_ndcg_k: 0.5753462286125012, test_ndcg_k: 0.42292322616281286\n",
      "n_tree: 84, train_ndcg_k: 0.5757916213750487, test_ndcg_k: 0.42311410169491326\n",
      "n_tree: 85, train_ndcg_k: 0.5760716227997909, test_ndcg_k: 0.42333100642108684\n",
      "n_tree: 86, train_ndcg_k: 0.5770385714703371, test_ndcg_k: 0.4217464883563476\n",
      "n_tree: 87, train_ndcg_k: 0.5757588941039934, test_ndcg_k: 0.4219226731088769\n",
      "n_tree: 88, train_ndcg_k: 0.5728673334550823, test_ndcg_k: 0.4208554812746586\n",
      "n_tree: 89, train_ndcg_k: 0.573380326363866, test_ndcg_k: 0.42133882059937106\n",
      "n_tree: 90, train_ndcg_k: 0.572736919847644, test_ndcg_k: 0.42183877014935267\n",
      "n_tree: 91, train_ndcg_k: 0.5729502925662991, test_ndcg_k: 0.4231177675649974\n",
      "n_tree: 92, train_ndcg_k: 0.5733206619466095, test_ndcg_k: 0.4217939725722435\n",
      "n_tree: 93, train_ndcg_k: 0.5728165638949461, test_ndcg_k: 0.4215458944010322\n",
      "n_tree: 94, train_ndcg_k: 0.5734764627537016, test_ndcg_k: 0.4194161446196469\n",
      "n_tree: 95, train_ndcg_k: 0.5731990739450372, test_ndcg_k: 0.4191445382531913\n",
      "n_tree: 96, train_ndcg_k: 0.5727144215406551, test_ndcg_k: 0.4175105436776715\n",
      "n_tree: 97, train_ndcg_k: 0.5736417214748881, test_ndcg_k: 0.4184202743936509\n",
      "n_tree: 98, train_ndcg_k: 0.574209935237835, test_ndcg_k: 0.41923917445754777\n",
      "n_tree: 99, train_ndcg_k: 0.574229960389831, test_ndcg_k: 0.41882526482472904\n",
      "CPU times: user 9min 1s, sys: 4.05 s, total: 9min 5s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tst_sol1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 0.43173542982162266)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_sol1.best_n_trees, tst_sol1.best_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tree: 0, train_ndcg_k: 0.45968059872384603, test_ndcg_k: 0.3295782293708683\n",
      "n_tree: 1, train_ndcg_k: 0.5216392964861525, test_ndcg_k: 0.336569784676915\n",
      "n_tree: 2, train_ndcg_k: 0.5221855200242471, test_ndcg_k: 0.36380267274701583\n",
      "n_tree: 3, train_ndcg_k: 0.5368954236716924, test_ndcg_k: 0.37187789290166084\n",
      "n_tree: 4, train_ndcg_k: 0.5403321922419148, test_ndcg_k: 0.3704574068159138\n",
      "n_tree: 5, train_ndcg_k: 0.5453128913862544, test_ndcg_k: 0.3725899192169184\n",
      "n_tree: 6, train_ndcg_k: 0.5485454855826337, test_ndcg_k: 0.3870515435122703\n",
      "CPU times: user 6.8 s, sys: 45.3 ms, total: 6.84 s\n",
      "Wall time: 3.61 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# tst_sol1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst_sol1.save_model(path='lambdamart_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_sol1.load_model(path='lambdamart_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst_sol1.save_model(path='lambdamart_02.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 0.4321047855828739)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# funs tests (draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = int(np.shape(tst_sol1.X_train)[0] * 0.9)\n",
    "N = np.shape(tst_sol1.X_train)[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[778, 127, 18, 845, 27, 423, 744, 182, 766, 725]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random()\n",
    "random.randint(0, 10)\n",
    "smpl1 = random.sample(range(1000), 10)\n",
    "smpl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3524, -0.4220,  0.4964,  ..., -0.1267, -0.0789, -0.4441],\n",
       "        [ 0.6456,  0.4900,  0.6141,  ...,  0.0000, -0.0974,  1.0093],\n",
       "        [ 0.3161, -0.2350, -0.3139,  ..., -0.1118, -0.1959, -0.2662],\n",
       "        ...,\n",
       "        [ 0.6456,  1.5499,  2.2621,  ...,  0.0000, -0.0977, -0.4654],\n",
       "        [ 0.3524, -0.4220,  0.4964,  ..., -0.1267, -0.0789, -0.4441],\n",
       "        [ 0.3524, -0.4220,  0.4964,  ..., -0.1267, -0.0789, -0.4441]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_sol1.X_train[smpl1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 55, 52, 57, 46, 97, 73, 39, 77,  6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = np.shape(tst_sol1.X_train[smpl1, ])\n",
    "np.array(random.sample(range(m), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[140260008804688,               0,               0],\n",
       "        [              0,               0,               0]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray([1,2,3], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6df22269584a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "np.ndarray([1,2,3], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', [1, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('a', [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(tst_sol1.train_preds == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(range(2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 136])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tst_sol1.X_train[random.sample(range(5000), 1000), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [1000], [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-00cde338e3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtst_sol1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# random.sample(range(100), 80)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [1000], [2]"
     ]
    }
   ],
   "source": [
    "tst_sol1.X_train[random.sample(range(5000), 1000), random.sample(range(10), 2)]\n",
    "# random.sample(range(100), 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad07ebab6b08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_sol1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "np.shape(tst_sol1.X_train[random.sample(range(5000), 1000), random.sample(range(10), 2)[:, np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2744,  1.2814,  1.2673,  0.4213,  0.7169],\n",
       "        [ 1.2463, -1.1928,  1.2915,  1.2466,  0.1801],\n",
       "        [ 1.9949, -0.0971,  0.2938, -2.0156,  1.6405],\n",
       "        [ 0.0980, -2.7853, -0.0406, -0.0128,  0.6291]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 5)\n",
    "x\n",
    "# torch.index_select(x, 1, (1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2814,  0.4213],\n",
       "        [-1.1928,  1.2466],\n",
       "        [-0.0971, -2.0156],\n",
       "        [-2.7853, -0.0128]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, [1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2814, -2.0156])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], [1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2814,  0.4213],\n",
       "        [-0.0971, -2.0156]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, [1, 3]][[0, 2], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2814,  0.4213],\n",
       "        [-0.0971, -2.0156]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, [1, 3]][[0, 2], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2814,  0.4213],\n",
       "        [-0.0971, -2.0156]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], :][:, [1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]), array([1, 2, 3])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "z.append(np.array([1,2,3]))\n",
    "z.append(np.array([1,2,3]))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 0., 2., 1., 1., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2.,\n",
       "        0., 0., 2., 2., 0., 1., 0., 1., 2., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 2., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 2., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t = torch.tensor([2., 2., 0., 2., 1., 1., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2.,\\\n",
    "        0., 0., 2., 2., 0., 1., 0., 1., 2., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\\\n",
    "        0., 0., 0., 0., 0., 1., 0., 0., 0., 2., 3., 0., 0., 0., 0., 0., 1., 0.,\\\n",
    "        0., 0., 2., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\\\n",
    "        0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 1., 1., 0.])\n",
    "tst_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 0., 2., 1., 1., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2.,\n",
       "        0., 0., 2., 2., 0., 1., 0., 1., 2., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 2., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 2., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [5.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tst_t2 = torch.LongTensor([[5, 3, 2, 5, 1, 1]]).reshape(-1, 1)\n",
    "tst_t2 = torch.Tensor([[5, 3, 2, 5, 1, 1]]).reshape(-1, 1)\n",
    "tst_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3., 2., 5., 1., 1.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t3 = torch.Tensor([5, 3, 2, 5, 1, 1])\n",
    "tst_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -2., -3.,  0., -4., -4.],\n",
       "        [ 2.,  0., -1.,  2., -2., -2.],\n",
       "        [ 3.,  1.,  0.,  3., -1., -1.],\n",
       "        [ 0., -2., -3.,  0., -4., -4.],\n",
       "        [ 4.,  2.,  1.,  4.,  0.,  0.],\n",
       "        [ 4.,  2.,  1.,  4.,  0.,  0.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t3 - tst_t3.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t3 - tst_t3.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(np.shape(tst_sol1.ys_train.reshape(1,-1)))\n",
    "torch.zeros(np.shape(tst_sol1.ys_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10000])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(torch.zeros(np.shape(tst_sol1.ys_train.reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [5.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t2 = torch.Tensor([[5, 3, 2, 5, 1, 1]]).reshape(-1, 1)\n",
    "tst_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3., 2., 5., 1., 1.])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_t2.reshape(1,-1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.]]).reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1,2,3,4,3,2,1])\n",
    "np.argmax([1,2,3,4,3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,2,3,4,3,4,4,4,4,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,3,4,4,4,4,2,1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 7, 5, 3, 6, 9, 4, 8, 0]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "sample_ids = random.sample(range(10), int(5 * 2))\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 0, 6, 3, 4, 8, 1, 7, 2, 5]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(10)\n",
    "sample_ids = random.sample(range(10), int(5 * 2))\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
